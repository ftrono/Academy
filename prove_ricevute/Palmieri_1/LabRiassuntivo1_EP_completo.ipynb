{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset corpus sample_corpus convertirlo in lista, in cui ogni frase Ã¨ una lista separata di parole\n",
        "\n",
        "\n",
        "*   contare numero di frasi, lunghezza tot del dataset in parole e lunghezza media di ogni frase\n",
        "*   Realizzare un vocabulary conteneti le parale univoche e la frequenza con cui sono nel dataset\n",
        "\n",
        "\n",
        "\n",
        "consegna: generare dei txt"
      ],
      "metadata": {
        "id": "eMe6TX7jouUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrfaV8Bmon6z",
        "outputId": "dfc041e1-e099-4490-f713-783f35df4166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/My Drive/Lab_Python-EP\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive, files\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=False)\n",
        "\n",
        "%cd '/content/drive/My Drive/Lab_Python-EP'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.**"
      ],
      "metadata": {
        "id": "Kj7OdYz169tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "dataset= './Dataset/sample_corpus.txt'\n",
        "dataset_word2index='./Out/sample_corpus_word2index.txt'\n",
        "dataset_index2word='./Out/sample_corpus_index2word.txt'\n",
        "\n",
        "lista_corpus=[]\n",
        "corpus=open(dataset,'r')\n",
        "#corpus_m=open(dataset_modified,'w')\n",
        "n_frasi=0\n",
        "l_dataset=0\n",
        "media_frase=[]\n",
        "\n",
        "for line in corpus:\n",
        "  #line_m=line.lower()\n",
        "  #line_m=line.strip().split()\n",
        "  frase=line.lower().strip().split()\n",
        "  lista_corpus.append(frase)\n",
        "  n_frasi+=1\n",
        "  l_dataset+=len(frase)\n",
        "  media_frase.append(len(frase))\n",
        "\n",
        "#print(n_frasi)\n",
        "#print(l_dataset)\n",
        "#print(statistics.mean(media_frase))\n"
      ],
      "metadata": {
        "id": "KbBc-36ws751"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.**"
      ],
      "metadata": {
        "id": "yzcLCn5J7CsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_parole_tot=[]\n",
        "lista_parole=[]\n",
        "\n",
        "for frase in lista_corpus:\n",
        "  for parola in frase:\n",
        "    lista_parole_tot.append(parola)\n",
        "    if parola not in lista_parole:\n",
        "      lista_parole.append(parola)\n",
        "\n",
        "#lista_parole\n",
        "#print(lista_parole.count('pay'))  #per verificare fosse 1\n",
        "\n",
        "vocabulary_parole={}\n",
        "for parola in lista_parole:\n",
        "  vocabulary_parole[parola]=lista_parole_tot.count(parola)\n",
        "\n",
        "#vocabulary_parole\n",
        "\n",
        "###len vocabulary:\n",
        "lunghezza_vocabulary_parole=len(vocabulary_parole)\n",
        "\n",
        "#print(lunghezza_vocabulary_parole)\n",
        "\n",
        "parole_fmin={k: vocabulary_parole[k] for k in vocabulary_parole.keys() if vocabulary_parole[k]==1}\n",
        "\n",
        "parole_fmax={k: vocabulary_parole[k] for k in vocabulary_parole.keys() if vocabulary_parole[k]==max(vocabulary_parole.values())}\n",
        "\n",
        "\n",
        "#parole_fmin\n",
        "#parole_fmax\n",
        "\n",
        "###crescenti/decrescenti\n",
        "\n",
        "lista_parole_crescente=sorted(lista_parole)\n",
        "vocabulary_parole_crescenti={}\n",
        "for parola in lista_parole_crescente:\n",
        "  vocabulary_parole_crescenti[parola]=lista_parole_tot.count(parola)\n",
        "\n",
        "\n",
        "lista_parole_decrescente=sorted(lista_parole, reverse=True)\n",
        "vocabulary_parole_decrescenti={}\n",
        "for parola in lista_parole_decrescente:\n",
        "  vocabulary_parole_decrescenti[parola]=lista_parole_tot.count(parola)\n",
        "#vocabulary_parole_crescente={sorted(k): vocabulary_parole[k] for k in vocabulary_parole.keys()}\n",
        "\n",
        "#vocabulary_parole_crescente\n",
        "#vocabulary_parole_decrescenti"
      ],
      "metadata": {
        "id": "CbiABwYO7CX9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.**"
      ],
      "metadata": {
        "id": "JZGpjAnXG4Ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_parole_f2={k: v for k,v in vocabulary_parole.items() if v >=2}\n",
        "#vocabulary_parole_f2\n",
        "\n",
        "vocabulary_parole_f100={k: v for k,v in vocabulary_parole_f2.items() if v <100}\n",
        "#vocabulary_parole_f100\n",
        "\n",
        "###stop_words\n",
        "dataset_stopwords= './Dataset/stop_words.txt'\n",
        "\n",
        "\n",
        "stop_words=open(dataset_stopwords,'r')\n",
        "stopwords=[]\n",
        "for el in stop_words:\n",
        "  stopwords.append(el.lower().strip())\n",
        "\n",
        "vocabulary_not_stopwords={k: vocabulary_parole[k] for k in vocabulary_parole_f100.keys() if k not in stopwords}\n",
        "#vocabulary_not_stopwords\n",
        "\n",
        "\n",
        "###lunghezze e min/max nuovi vocabulary\n",
        "\n",
        "lunghezza_vocabulary_parole_f2=len(vocabulary_parole_f2)\n",
        "#print(lunghezza_vocabulary_parole_f2)\n",
        "parole_f2_fmin={k: vocabulary_parole_f2[k] for k in vocabulary_parole_f2.keys() if vocabulary_parole_f2[k]==min(vocabulary_parole_f2.values())}\n",
        "#parole_f2_fmin\n",
        "\n",
        "lunghezza_vocabulary_parole_f100=len(vocabulary_parole_f100)\n",
        "#print(lunghezza_vocabulary_parole_f100)\n",
        "parole_f100_fmax={k: vocabulary_parole_f100[k] for k in vocabulary_parole_f100.keys() if vocabulary_parole_f100[k]==max(vocabulary_parole_f2.values())}\n",
        "#parole_f100_fmax\n",
        "\n",
        "lunghezza_vocabulary_not_stopwords=len(vocabulary_not_stopwords)\n",
        "print(lunghezza_vocabulary_not_stopwords)\n",
        "stopwords_fmin={k: vocabulary_not_stopwords[k] for k in vocabulary_not_stopwords.keys() if vocabulary_not_stopwords[k]==min(vocabulary_not_stopwords.values())}\n",
        "stopwords_fmax={k: vocabulary_not_stopwords[k] for k in vocabulary_not_stopwords.keys() if vocabulary_not_stopwords[k]==max(vocabulary_not_stopwords.values())}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo8vS7KeG3dC",
        "outputId": "e688a9bd-eea2-4a7b-d7da-0b10ba9d2d68"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.**"
      ],
      "metadata": {
        "id": "L8RbRl6IaHAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#n_lista_corpus=[]\n",
        "\n",
        "def word2index(voc,lista):\n",
        "  n_lista=[]\n",
        "  v_list=list(voc.keys())\n",
        "  for lis in lista:\n",
        "    vettore=[]\n",
        "    for el in lis:\n",
        "        if el in  v_list:\n",
        "          vettore.append(v_list.index(el))\n",
        "        else: vettore.append('-1')\n",
        "    n_lista.append(vettore)\n",
        "  return n_lista\n",
        "\n",
        "\n",
        "\n",
        "lista_word2index= word2index(vocabulary_not_stopwords, lista_corpus)\n"
      ],
      "metadata": {
        "id": "wg4C8skoaJox"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def index2word(voc,lista):\n",
        "  n_lista=[]\n",
        "  v_list=list(voc.keys())\n",
        "  for lis in lista:\n",
        "    vettore=[]\n",
        "    for el in lis:\n",
        "      if el=='-1':\n",
        "        vettore.append('OOV')\n",
        "      else: vettore.append(v_list[el])\n",
        "    n_lista.append(vettore)\n",
        "  return n_lista\n",
        "\n",
        "#print(index2word(vocabulary_not_stopwords,lista_word2index))\n",
        "lista_index2word=index2word(vocabulary_not_stopwords, lista_word2index)"
      ],
      "metadata": {
        "id": "Q5E3GYF5ITWb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_dataset_word2index=open(dataset_word2index,'w')\n",
        "out_dataset_index2word= open(dataset_index2word, 'w')\n",
        "\n",
        "riga_w2i=''\n",
        "for el in lista_word2index:\n",
        "  riga_w2i+=str(lista_word2index)+'\\n'\n",
        "\n",
        "with out_dataset_word2index as temp:\n",
        "  temp.write(riga_w2i)\n",
        "\n",
        "\n",
        "riga_i2w=''\n",
        "for el in lista_index2word:\n",
        "  riga_i2w=str(lista_index2word)+'\\n'\n",
        "\n",
        "with out_dataset_index2word as temp:\n",
        "  temp.write(riga_i2w)\n",
        "\n",
        "out_dataset_word2index.close()\n",
        "out_dataset_index2word.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "xbB6FWiKKVMY"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}