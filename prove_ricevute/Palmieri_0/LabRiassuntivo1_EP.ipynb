{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset corpus sample_corpus convertirlo in lista, in cui ogni frase Ã¨ una lista separata di parole\n",
        "\n",
        "\n",
        "*   contare numero di frasi, lunghezza tot del dataset in parole e lunghezza media di ogni frase\n",
        "*   Realizzare un vocabulary conteneti le parale univoche e la frequenza con cui sono nel dataset\n",
        "\n",
        "\n",
        "\n",
        "consegna: generare dei txt"
      ],
      "metadata": {
        "id": "eMe6TX7jouUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrfaV8Bmon6z",
        "outputId": "5214cbcf-5a68-4a52-de7b-c48427d7f9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/My Drive/Lab_Python-EP\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive, files\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=False)\n",
        "\n",
        "%cd '/content/drive/My Drive/Lab_Python-EP'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.**"
      ],
      "metadata": {
        "id": "Kj7OdYz169tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "dataset= './Dataset/sample_corpus.txt'\n",
        "#dataset_modified='./Out/sample_corpus.txt'\n",
        "\n",
        "lista_corpus=[]\n",
        "corpus=open(dataset,'r')\n",
        "#corpus_m=open(dataset_modified,'w')\n",
        "n_frasi=0\n",
        "l_dataset=0\n",
        "media_frase=[]\n",
        "\n",
        "for line in corpus:\n",
        "  #line_m=line.lower()\n",
        "  #line_m=line.strip().split()\n",
        "  frase=line.lower().strip().split()\n",
        "  lista_corpus.append(frase)\n",
        "  n_frasi+=1\n",
        "  l_dataset+=len(frase)\n",
        "  media_frase.append(len(frase))\n",
        "\n",
        "#print(n_frasi)\n",
        "#print(l_dataset)\n",
        "#print(statistics.mean(media_frase))\n"
      ],
      "metadata": {
        "id": "KbBc-36ws751"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.**"
      ],
      "metadata": {
        "id": "yzcLCn5J7CsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_parole_tot=[]\n",
        "lista_parole=[]\n",
        "\n",
        "for frase in lista_corpus:\n",
        "  for parola in frase:\n",
        "    lista_parole_tot.append(parola)\n",
        "    if parola not in lista_parole:\n",
        "      lista_parole.append(parola)\n",
        "\n",
        "#lista_parole\n",
        "#print(lista_parole.count('pay'))  #per verificare fosse 1\n",
        "\n",
        "vocabulary_parole={}\n",
        "for parola in lista_parole:\n",
        "  vocabulary_parole[parola]=lista_parole_tot.count(parola)\n",
        "\n",
        "#vocabulary_parole\n",
        "\n",
        "###len vocabulary:\n",
        "lunghezza_vocabulary_parole=len(vocabulary_parole)\n",
        "\n",
        "#print(lunghezza_vocabulary_parole)\n",
        "\n",
        "parole_fmin={k: vocabulary_parole[k] for k in vocabulary_parole.keys() if vocabulary_parole[k]==1}\n",
        "\n",
        "parole_fmax={k: vocabulary_parole[k] for k in vocabulary_parole.keys() if vocabulary_parole[k]==max(vocabulary_parole.values())}\n",
        "\n",
        "\n",
        "#parole_fmin\n",
        "#parole_fmax\n",
        "\n",
        "###crescenti/decrescenti\n",
        "\n",
        "lista_parole_crescente=sorted(lista_parole)\n",
        "vocabulary_parole_crescenti={}\n",
        "for parola in lista_parole_crescente:\n",
        "  vocabulary_parole_crescenti[parola]=lista_parole_tot.count(parola)\n",
        "\n",
        "\n",
        "lista_parole_decrescente=sorted(lista_parole, reverse=True)\n",
        "vocabulary_parole_decrescenti={}\n",
        "for parola in lista_parole_decrescente:\n",
        "  vocabulary_parole_decrescenti[parola]=lista_parole_tot.count(parola)\n",
        "#vocabulary_parole_crescente={sorted(k): vocabulary_parole[k] for k in vocabulary_parole.keys()}\n",
        "\n",
        "#vocabulary_parole_crescente\n",
        "#vocabulary_parole_decrescenti"
      ],
      "metadata": {
        "id": "CbiABwYO7CX9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.**"
      ],
      "metadata": {
        "id": "JZGpjAnXG4Ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_parole_f2={k: v for k,v in vocabulary_parole.items() if v >=2}\n",
        "#vocabulary_parole_f2\n",
        "\n",
        "vocabulary_parole_f100={k: v for k,v in vocabulary_parole_f2.items() if v <100}\n",
        "#vocabulary_parole_f100\n",
        "\n",
        "###stop_words\n",
        "dataset_stopwords= './Dataset/stop_words.txt'\n",
        "\n",
        "\n",
        "stop_words=open(dataset_stopwords,'r')\n",
        "stopwords=[]\n",
        "for el in stop_words:\n",
        "  stopwords.append(el.lower().strip())\n",
        "\n",
        "vocabulary_not_stopwords={k: vocabulary_parole[k] for k in vocabulary_parole_f100.keys() if k not in stopwords}\n",
        "#vocabulary_not_stopwords\n",
        "\n",
        "\n",
        "###lunghezze e min/max nuovi vocabulary\n",
        "\n",
        "lunghezza_vocabulary_parole_f2=len(vocabulary_parole_f2)\n",
        "#print(lunghezza_vocabulary_parole_f2)\n",
        "parole_f2_fmin={k: vocabulary_parole_f2[k] for k in vocabulary_parole_f2.keys() if vocabulary_parole_f2[k]==min(vocabulary_parole_f2.values())}\n",
        "#parole_f2_fmin\n",
        "\n",
        "lunghezza_vocabulary_parole_f100=len(vocabulary_parole_f100)\n",
        "#print(lunghezza_vocabulary_parole_f100)\n",
        "parole_f100_fmax={k: vocabulary_parole_f100[k] for k in vocabulary_parole_f100.keys() if vocabulary_parole_f100[k]==max(vocabulary_parole_f2.values())}\n",
        "#parole_f100_fmax\n",
        "\n",
        "lunghezza_vocabulary_not_stopwords=len(vocabulary_not_stopwords)\n",
        "print(lunghezza_vocabulary_not_stopwords)\n",
        "stopwords_fmin={k: vocabulary_not_stopwords[k] for k in vocabulary_not_stopwords.keys() if vocabulary_not_stopwords[k]==min(vocabulary_not_stopwords.values())}\n",
        "stopwords_fmax={k: vocabulary_not_stopwords[k] for k in vocabulary_not_stopwords.keys() if vocabulary_not_stopwords[k]==max(vocabulary_not_stopwords.values())}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo8vS7KeG3dC",
        "outputId": "4dbd68bc-142b-45bd-d889-0fac89108fc0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.**"
      ],
      "metadata": {
        "id": "L8RbRl6IaHAF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wg4C8skoaJox"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}